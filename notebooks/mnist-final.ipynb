{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/16\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.4310 - acc: 0.8832 - val_loss: 0.2645 - val_acc: 0.9240\n",
      "Epoch 2/16\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.2249 - acc: 0.9369 - val_loss: 0.1965 - val_acc: 0.9416\n",
      "Epoch 3/16\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.1751 - acc: 0.9512 - val_loss: 0.1579 - val_acc: 0.9546\n",
      "Epoch 4/16\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1442 - acc: 0.9587 - val_loss: 0.1345 - val_acc: 0.9632\n",
      "Epoch 5/16\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1228 - acc: 0.9642 - val_loss: 0.1312 - val_acc: 0.9644\n",
      "Epoch 6/16\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.1072 - acc: 0.9687 - val_loss: 0.1158 - val_acc: 0.9677\n",
      "Epoch 7/16\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0953 - acc: 0.9725 - val_loss: 0.1134 - val_acc: 0.9686\n",
      "Epoch 8/16\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.0857 - acc: 0.9753 - val_loss: 0.1074 - val_acc: 0.9699\n",
      "Epoch 9/16\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0788 - acc: 0.9769 - val_loss: 0.1029 - val_acc: 0.9724\n",
      "Epoch 10/16\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0723 - acc: 0.9793 - val_loss: 0.1027 - val_acc: 0.9729\n",
      "Epoch 11/16\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0670 - acc: 0.9805 - val_loss: 0.0973 - val_acc: 0.9733\n",
      "Epoch 12/16\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0622 - acc: 0.9816 - val_loss: 0.0979 - val_acc: 0.9729\n",
      "Epoch 13/16\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0582 - acc: 0.9829 - val_loss: 0.0962 - val_acc: 0.9728\n",
      "Epoch 14/16\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0537 - acc: 0.9841 - val_loss: 0.0922 - val_acc: 0.9761\n",
      "Epoch 15/16\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.0505 - acc: 0.9849 - val_loss: 0.1021 - val_acc: 0.9733\n",
      "Epoch 16/16\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0475 - acc: 0.9860 - val_loss: 0.0964 - val_acc: 0.9746\n",
      "Test loss: 0.09636682539703324\n",
      "Test accuracy: 0.9746\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class NormalDensity(layers.Layer):\n",
    "\n",
    "    def __init__(self, output_dim, batch_size, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batch_size\n",
    "        super(NormalDensity, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        #Define set of traditional weights\n",
    "        self.w = self.add_weight(name='w', \n",
    "                                 shape=(input_shape[1], self.output_dim),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        \n",
    "        #Define our plasticity coefficient\n",
    "        self.alpha = self.add_weight(name='alpha', \n",
    "                              shape=(1, 1),\n",
    "                              initializer='uniform',\n",
    "                              trainable=True)\n",
    "        \n",
    "        \n",
    "        #The Hebbian trace\n",
    "        self.hebb = self.add_weight(name='hebb', \n",
    "                              shape=(input_shape[1], self.output_dim),\n",
    "                              initializer='zeros',\n",
    "                              trainable=False)\n",
    "        \n",
    "        #Step size will be optimized\n",
    "        self.eta = self.add_weight(name='eta', \n",
    "                                      shape=(1, 1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        super(NormalDensity, self).build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        #X (layer input)     : shape(?, INPUT_DIM)\n",
    "        #W                   : shape(INPUT_DIM, OUTPUT_DIM)\n",
    "        #hebb                : shape(INPUT_DIM, OUTPUT_DIM)\n",
    "        #Y (layer output)    : shape(?, OUTPUT_DIM)\n",
    "        #ETA                 : scalar (one per layer)\n",
    "                \n",
    "        y = K.dot(x, self.w)\n",
    "        plastic_y = self.alpha * (K.dot(x, self.hebb))   \n",
    "        model_out = K.maximum(0.0, y + plastic_y)\n",
    "        \n",
    "        #Hebbian update - option 1\n",
    "        self.hebb = self.eta * K.dot(x, model_out) + (1 - self.eta) * self.hebb\n",
    "        \n",
    "        #Hebbian update - option 2\n",
    "        #self.hebb +=self.eta * K.dot(model_out, (x - (K.dot(model_out, self.hebb))))\n",
    "\n",
    "        return model_out\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 16\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Our layer using DP\n",
    "#model.add(NormalDensity(50, batch_size, input_shape=(784,)))\n",
    "\n",
    "#Traditional dense layer\n",
    "model.add(Dense(50, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
