{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Classic: n: 4 l: 4 e: 2\n",
      "WARNING:tensorflow:From /home/mun/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:125: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=11, units=4)`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:127: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=4, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "/home/mun/.local/lib/python2.7/site-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have shape (None, 11) but got array with shape (60000, 784)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1438e40b9454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m#extensive_test()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestClassic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestPlastic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1438e40b9454>\u001b[0m in \u001b[0;36mtestClassic\u001b[0;34m(num_neurons, num_layers, epochs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mun/.local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/mun/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1582\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mun/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1412\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1415\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1416\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mun/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    151\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have shape (None, 11) but got array with shape (60000, 784)"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Trains a simple deep NN on the MNIST dataset.\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "(there is *a lot* of margin for parameter tuning).\n",
    "2 seconds per epoch on a K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class PlasticDense(layers.Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        #self.batch_size = batch_size\n",
    "        super(PlasticDense, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        #Define set of traditional weights\n",
    "        self.w = self.add_weight(name='w', \n",
    "                                 shape=(input_shape[1], self.output_dim),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        \n",
    "        #Define our plasticity coefficient\n",
    "        self.alpha = self.add_weight(name='alpha', \n",
    "                              shape=(1, 1),\n",
    "                              initializer='uniform',\n",
    "                              trainable=True)\n",
    "        \n",
    "        \n",
    "        #The Hebbian trace\n",
    "        self.hebb = self.add_weight(name='hebb', \n",
    "                              shape=(input_shape[1], self.output_dim),\n",
    "                              initializer='zeros',\n",
    "                              trainable=False)\n",
    "        \n",
    "        #Step size will be optimized\n",
    "        self.eta = self.add_weight(name='eta', \n",
    "                                      shape=(1, 1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        super(PlasticDense, self).build(input_shape)\n",
    "\n",
    "        \n",
    "        #yout = F.tanh( yin.mm(self.w + torch.mul(self.alpha, hebb)) + input )\n",
    "        #hebb = (1 - self.eta) * hebb + self.eta * torch.bmm(yin.unsqueeze(2), yout.unsqueeze(1))[0] # bmm here is used to implement an outer product between yin and yout, with the help of unsqueeze (i.e. added empty dimensions)\n",
    "        #return yout, hebb\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        #X (layer input)     : shape(?, INPUT_DIM)\n",
    "        #W                   : shape(INPUT_DIM, OUTPUT_DIM)\n",
    "        #hebb                : shape(INPUT_DIM, OUTPUT_DIM)\n",
    "        #Y (layer output)    : shape(?, OUTPUT_DIM)\n",
    "        #ETA                 : scalar (one per layer)\n",
    "        \n",
    "        #yout = K.maximum(0.0, np.add((K.dot(self.y, np.add(K.dot(self.alpha, K.transpose(self.hebb)), self.w))), x))\n",
    "        #hebb = (1 - 0.01) * self.hebb + 0.01 * K.dot(self.y, yout)\n",
    "        #yout = K.maximum(0.0, np.add(self.y * np.add(self.alpha * self.hebb, self.w), x))\n",
    "        \n",
    "        #y = K.dot(x, self.w)\n",
    "        #y = F.tanh( yin.mm(self.w + torch.mul(self.alpha, hebb)) + yin )\n",
    "        #plastic_y = self.alpha * (K.dot(x, self.hebb)) \n",
    "        #self.hebb = (1 - self.eta) * self.hebb + self.eta * torch.bmm(yin.unsqueeze(2), yout.unsqueeze(1))[0]\n",
    "        #model_out = K.maximum(0.0, y + plastic_y)\n",
    "        \n",
    "        #Hebbian update - option 1\n",
    "        #self.hebb = self.eta * K.dot(x, model_out) + (1 - self.eta) * self.hebb\n",
    "        #print(self.hebb)\n",
    "        \n",
    "        #Hebbian update - option 2\n",
    "        #self.hebb +=self.eta * K.dot(model_out, (x - (K.dot(model_out, self.hebb))))\n",
    "        \n",
    "        y = K.dot(x, self.w)\n",
    "        plastic_y = self.alpha * (K.dot(x, self.hebb))   \n",
    "        model_out = K.maximum(0.0, y + plastic_y)\n",
    "        \n",
    "        #Hebbian update - option 1\n",
    "        self.hebb = self.eta * K.dot(x, model_out) + (1 - self.eta) * self.hebb\n",
    "        print(self.hebb)\n",
    "\n",
    "        return model_out\n",
    "        #return K.maximum(0.0, y)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 16\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "def testClassic(num_neurons, num_layers, epochs):\n",
    "    print(\"Classic: n: {} l: {} e: {}\".format(num_neurons, num_layers, epochs))\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(output_dim = num_neurons, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    for l in range(num_layers - 1):\n",
    "        classifier.add(Dense(output_dim = num_neurons, init = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    train_history = classifier.fit(x_train, y_train, batch_size = 128, nb_epoch = epochs)\n",
    "    \n",
    "    loss_history = train_history.history[\"loss\"]\n",
    "    np.savetxt(\"results/n_{}_l_{}_e_{}_loss_classic.txt\".format(num_neurons, num_layers, epochs), \n",
    "                  np.array(loss_history), delimiter=\",\")\n",
    "    \n",
    "    acc_history = train_history.history[\"acc\"]\n",
    "    np.savetxt(\"results/n_{}_l_{}_e_{}_acc_classic.txt\".format(num_neurons, num_layers, epochs), \n",
    "                  np.array(acc_history), delimiter=\",\")\n",
    "    # TODO Append test prediction results to file or to its own file\n",
    "    #y_pred = classifier.predict(X_test)\n",
    "    #return (y_pred > 0.5)\n",
    "\n",
    "\n",
    "def testPlastic(num_neurons, num_layers, epochs):\n",
    "    print(\"Plastic: n: {} l: {} e: {}\".format(num_neurons, num_layers, epochs))\n",
    "    classifier = Sequential()\n",
    "    classifier.add(PlasticDense(num_neurons, input_shape=(11,)))\n",
    "    for l in range(num_layers - 1):\n",
    "        classifier.add(PlasticDense(num_neurons))\n",
    "    classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    train_history = classifier.fit(x_train, y_train, batch_size = 128, nb_epoch = epochs)\n",
    "        \n",
    "    loss_history = train_history.history[\"loss\"]\n",
    "    np.savetxt(\"results/n_{}_l_{}_e_{}_loss_plastic.txt\".format(num_neurons, num_layers, epochs), \n",
    "                  np.array(loss_history), delimiter=\",\")\n",
    "    \n",
    "    acc_history = train_history.history[\"acc\"]\n",
    "    np.savetxt(\"results/n_{}_l_{}_e_{}_acc_plastic.txt\".format(num_neurons, num_layers, epochs), \n",
    "                  np.array(acc_history), delimiter=\",\")\n",
    "    # TODO Append test prediction results to file or to its own file\n",
    "   # y_pred = classifier.predict(X_test)\n",
    "   # return (y_pred > 0.5)\n",
    "    \n",
    "    \n",
    "def extensive_test():\n",
    "    neuron_combos = [4, 6, 8, 12, 16]\n",
    "    layer_combos = [2, 4, 8, 16]\n",
    "    epochs = 200\n",
    "    \n",
    "    i = 0\n",
    "    for n in neuron_combos:\n",
    "        for l in layer_combos:\n",
    "            testClassic(n, l, epochs)\n",
    "            testPlastic(n, l, epochs)\n",
    "            i += 1\n",
    "            print(\"Try: {}/{}\".format(i, len(neuron_combos)*len(layer_combos)))\n",
    "\n",
    "#extensive_test()\n",
    "y_pred = testClassic(4, 4, 2)\n",
    "y_pred = testPlastic(4, 4, 20)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "#score = model.evaluate(x_test, y_test, verbose=0)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
