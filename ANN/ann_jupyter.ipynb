{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Neural Network\n",
    "\n",
    "# Installing Theano\n",
    "# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
    "\n",
    "# Installing Tensorflow\n",
    "# Install Tensorflow from the website: https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html\n",
    "\n",
    "# Installing Keras\n",
    "# \n",
    "\n",
    "\n",
    "# Part 1 - Data Preprocessing\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trains a simple deep NN on the MNIST dataset.\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "(there is *a lot* of margin for parameter tuning).\n",
    "2 seconds per epoch on a K520 GPU.\n",
    "'''\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#from torch.autograd import Variable\n",
    "#import torch.nn.functional as F\n",
    "#from torch import optim\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class PlasticDense(layers.Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        #self.batch_size = batch_size\n",
    "        super(PlasticDense, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        #Define set of traditional weights\n",
    "        self.w = self.add_weight(name='w', \n",
    "                                 shape=(input_shape[1], self.output_dim),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        \n",
    "        #Define our plasticity coefficient\n",
    "        self.alpha = self.add_weight(name='alpha', \n",
    "                              shape=(1, 1),\n",
    "                              initializer='uniform',\n",
    "                              trainable=True)\n",
    "        \n",
    "        \n",
    "        #The Hebbian trace\n",
    "        self.hebb = self.add_weight(name='hebb', \n",
    "                              shape=(input_shape[1], self.output_dim),\n",
    "                              initializer='zeros',\n",
    "                              trainable=False)\n",
    "        \n",
    "        #Step size will be optimized\n",
    "        self.eta = self.add_weight(name='eta', \n",
    "                                      shape=(1, 1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        super(PlasticDense, self).build(input_shape)\n",
    "\n",
    "        \n",
    "        #yout = F.tanh( yin.mm(self.w + torch.mul(self.alpha, hebb)) + input )\n",
    "        #hebb = (1 - self.eta) * hebb + self.eta * torch.bmm(yin.unsqueeze(2), yout.unsqueeze(1))[0] # bmm here is used to implement an outer product between yin and yout, with the help of unsqueeze (i.e. added empty dimensions)\n",
    "        #return yout, hebb\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        #X (layer input)     : shape(?, INPUT_DIM)\n",
    "        #W                   : shape(INPUT_DIM, OUTPUT_DIM)\n",
    "        #hebb                : shape(INPUT_DIM, OUTPUT_DIM)\n",
    "        #Y (layer output)    : shape(?, OUTPUT_DIM)\n",
    "        #ETA                 : scalar (one per layer)\n",
    "        \n",
    "        #yout = K.maximum(0.0, np.add((K.dot(self.y, np.add(K.dot(self.alpha, K.transpose(self.hebb)), self.w))), x))\n",
    "        #hebb = (1 - 0.01) * self.hebb + 0.01 * K.dot(self.y, yout)\n",
    "        #yout = K.maximum(0.0, np.add(self.y * np.add(self.alpha * self.hebb, self.w), x))\n",
    "        \n",
    "        #y = K.dot(x, self.w)\n",
    "        #y = F.tanh( yin.mm(self.w + torch.mul(self.alpha, hebb)) + yin )\n",
    "        #plastic_y = self.alpha * (K.dot(x, self.hebb)) \n",
    "        #self.hebb = (1 - self.eta) * self.hebb + self.eta * torch.bmm(yin.unsqueeze(2), yout.unsqueeze(1))[0]\n",
    "        #model_out = K.maximum(0.0, y + plastic_y)\n",
    "        \n",
    "        #Hebbian update - option 1\n",
    "        #self.hebb = self.eta * K.dot(x, model_out) + (1 - self.eta) * self.hebb\n",
    "        #print(self.hebb)\n",
    "        \n",
    "        #Hebbian update - option 2\n",
    "        #self.hebb +=self.eta * K.dot(model_out, (x - (K.dot(model_out, self.hebb))))\n",
    "        \n",
    "        y = K.dot(x, self.w)\n",
    "        plastic_y = self.alpha * (K.dot(x, self.hebb))   \n",
    "        model_out = K.maximum(0.0, y + plastic_y)\n",
    "        \n",
    "        #Hebbian update - option 1\n",
    "        self.hebb = self.eta * K.dot(x, model_out) + (1 - self.eta) * self.hebb\n",
    "        print(self.hebb)\n",
    "\n",
    "        return model_out\n",
    "        #return K.maximum(0.0, y)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def testClassic(num_neurons, num_layers, epochs):\n",
    "    print(\"Classic: n: {} l: {} e: {}\".format(num_neurons, num_layers, epochs))\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(output_dim = num_neurons, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    for l in range(num_layers - 1):\n",
    "        classifier.add(Dense(output_dim = num_neurons, init = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    train_history = classifier.fit(X_train, y_train, batch_size = 128, nb_epoch = epochs)\n",
    "    \n",
    "    loss_history = train_history.history[\"loss\"]\n",
    "    np.savetxt(\"results/n_{}_l_{}_e_{}_loss_classic.txt\".format(num_neurons, num_layers, epochs), \n",
    "                  np.array(loss_history), delimiter=\",\")\n",
    "    \n",
    "    acc_history = train_history.history[\"acc\"]\n",
    "    np.savetxt(\"results/n_{}_l_{}_e_{}_acc_classic.txt\".format(num_neurons, num_layers, epochs), \n",
    "                  np.array(acc_history), delimiter=\",\")\n",
    "    # TODO Append test prediction results to file or to its own file\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    return (y_pred > 0.5)\n",
    "\n",
    "\n",
    "def testPlastic(num_neurons, num_layers, epochs):\n",
    "    print(\"Plastic: n: {} l: {} e: {}\".format(num_neurons, num_layers, epochs))\n",
    "    classifier = Sequential()\n",
    "    classifier.add(PlasticDense(num_neurons, input_shape=(11,)))\n",
    "    for l in range(num_layers - 1):\n",
    "        classifier.add(PlasticDense(num_neurons))\n",
    "    classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    train_history = classifier.fit(X_train, y_train, batch_size = 128, nb_epoch = epochs)\n",
    "        \n",
    "    loss_history = train_history.history[\"loss\"]\n",
    "    np.savetxt(\"results/n_{}_l_{}_e_{}_loss_plastic.txt\".format(num_neurons, num_layers, epochs), \n",
    "                  np.array(loss_history), delimiter=\",\")\n",
    "    \n",
    "    acc_history = train_history.history[\"acc\"]\n",
    "    np.savetxt(\"results/n_{}_l_{}_e_{}_acc_plastic.txt\".format(num_neurons, num_layers, epochs), \n",
    "                  np.array(acc_history), delimiter=\",\")\n",
    "    # TODO Append test prediction results to file or to its own file\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    return (y_pred > 0.5)\n",
    "    \n",
    "    \n",
    "def extensive_test():\n",
    "    neuron_combos = [4, 6, 8, 12, 16]\n",
    "    layer_combos = [2, 4, 8, 16]\n",
    "    epochs = 200\n",
    "    \n",
    "    i = 0\n",
    "    for n in neuron_combos:\n",
    "        for l in layer_combos:\n",
    "            testClassic(n, l, epochs)\n",
    "            testPlastic(n, l, epochs)\n",
    "            i += 1\n",
    "            print(\"Try: {}/{}\".format(i, len(neuron_combos)*len(layer_combos)))\n",
    "\n",
    "#extensive_test()\n",
    "#y_pred = testClassic(4, 4, 2)\n",
    "y_pred = testPlastic(4, 4, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
